<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Python 基础爬虫（二）Requests库]]></title>
    <url>%2F2020%2F06%2F04%2Fpython%2Frequests%2F</url>
    <content type="text"><![CDATA[什么是 Requests 库&ensp;&ensp;&ensp;&ensp;Requests 是一常用的 http 请求库，它使用 python 语言编写，可以方便地发送 http 请求，以及方便地处理响应结果。 运行环境&ensp;&ensp;&ensp;&ensp;博主使用的是 IDE 系列的 PyCharm 一个很好用的神器（不是打广告！！是真的好用！！！） 安装方式&ensp;&ensp;&ensp;&ensp;1.windows + R 输入 cmd 右键管理员运行，输入 pip insrall requests&ensp;&ensp;&ensp;&ensp;2.使用 Anaconda 版本的朋友 在命令窗口输入 conda install requests&ensp;&ensp;&ensp;&ensp;3.使用 PyCharm 进行安装 file-》default settings-》project interpreter-》搜索 requests-》install package-》ok 使用方式&ensp;&ensp;&ensp;&ensp;打开我们的 PyCharm 工具，新建一个 Python 文件，在目录右键新建一个 Python 文件，然后输入文件名称，然后回车就行了 先写一个最简单的爬虫（GET 请求）&ensp;&ensp;&ensp;&ensp;我们的第一行代码首先是导入刚刚引入的 requests 库 import requests &ensp;&ensp;&ensp;&ensp;输入想要访问的目标网址并且获取他的响应状态，直接输入 requests.get(&#39;目标网址&#39;) 默认返回的是响应状态和响应对象 requests.get('http://www.baidu.com') # 要访问的目标网址 print(requests.get('http://www.baidu.com')) # 输出目标网址的响应状态 &ensp;&ensp;&ensp;&ensp;完整代码如下，这个 if __name__ == &quot;__main__&quot;: 相当于 Java 里边的 main 方法 import requests if __name__ == "__main__": #指定请求的url地址 url = 'http://www.baidu.com' #使用requests模块的get函数根据指定的url发起一个get请求，get函数返回一个响应对象 response = requests.get(url) #打印响应对象（结果：响应对象类型和响应状态码） print(response) 属性介绍&ensp;&ensp;&ensp;&ensp;使用 requests 模块向百度首页面发起一个 get 请求，获取响应对象 import requests if __name__ == "__main__": #指定请求的url url = 'http://www.baidu.com' #使用requests模块的get函数根据指定的url发起一个get请求，get函数返回一个响应对象 response = requests.get(url) #获取请求的url print('请求的url:'+response.url) #获取响应状态码 print(response.status_code) #获取响应内容的编码格式,可以通过该属性修改响应的编码格式，直接以赋值的形式就可以修改响应的编码方式，一般出现乱码是我们需要进行设置 print('响应内容编码：'+response.encoding) #获取响应对象的响应头信息： print(response.headers) #获取字符串形式的响应内容，即是我们通过审查元素看到的HTML内容 print(response.text) #获取字节形式的响应内容，是bytes类型，一般我们请求图频、音频、视频需要用到字节流形式的响应内容 print(response.content) 设置响应头&ensp;&ensp;&ensp;&ensp;在请求头中有一个参数为User-Agent，表示含义为请求载体的身份标识。通过浏览器发起的请求，请求载体为浏览器，则该请求的User-Agent为浏览器的身份标识，使用爬虫程序发起的请求，则该请求的载体为爬虫程序，则该请求的User-Agent为爬虫程序的身份标识。爬虫程序想要尽可能的模拟浏览器发起的请求，则必须将User-Agent修改成浏览器的身份标识。 注意：User-Agent的定制其实是一种反反爬虫的初级技术手段。所以在编写爬虫程序时，必须对User-Agent进行手动定制。User-Agent的值可以通过抓包工具从浏览器请求的请求头中获取。 import requests if __name__ == "__main__": #指定请求的url url = 'http://www.baidu.com' #定制请求头信息，相关的头信息必须封装在字典结构中 headers = { #定制请求头中的User-Agent参数，当然也可以定制请求头中其他的参数 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36', } #使用requests模块的get函数根据指定的url发起一个get请求，get函数返回一个响应对象 #参数1：发起请求指定的url #参数2：手动定制的请求头信息 response = requests.get(url=url,headers=headers) #打印响应的状态码 print(response.status_code) 携带参数的 GET 请求注意：url有一个显著的特性。url必须是使用ASCII进行编码的字符方可生效。但是在requests模块中，即使url中存在非ASCII字符，那么requests会对其自动进行转化。 import requests if __name__ == "__main__": #指定请求的url：可以将get请求携带的参数拼接到url域名后面，但是不建议大家这么做，一般我们将携带的参数以键值对的形式封装到字典中，操作见下方： #url = 'http://www.baidu.com/s?ie=utf-8&amp;wd=周杰伦' #建议做法：指定请求网页的域名 url = 'https://www.baidu.com/s' #将请求携带的参数以键值对的形式封装到字典中，准备待用 param = { 'ie' : 'utf-8', 'wd' : '星星' } #定制请求头信息，相关的头信息必须封装在字典结构中 headers = { #定制请求头中的User-Agent参数，当然也可以定制请求头中其他的参数 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36', } #使用requests模块的get函数根据指定的url发起一个get请求，get函数返回一个响应对象，get中传入的三个参数如下： #参数1：发起请求指定的url #参数2：手动定制的请求头信息 #参数3：指定请求携带的参数 response = requests.get(url=url,headers=headers,params=param) #设置响应内容的编码格式（如果写入文件内容出现乱码的话） response.encoding = 'utf-8' #获取响应的网页内容，存储的文件名我们也可以根据用户输入，存成一个动态文件名 page_data = response.text with open('./星星.html','w') as fp: fp.write(page_data) 写一个最简单的爬虫（POST 请求）携带参数的 POST 请求注意：这个地方有一个小坑，使用 GET 方法带参数请求时，是 params=参数字典，而不是 data，data 是POST 方法的参数 import requests if __name__ == "__main__": #指定post请求的url url = 'https://fanyi.baidu.com/sug' #定制请求头信息，相关的头信息必须封装在字典结构中 headers = { #定制请求头中的User-Agent参数，将爬虫伪装的更像浏览器，当然也可以定制请求头中其他的参数 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36', } #定制post请求携带的参数,可以使用抓包工具看需要传哪些参数，如Fiddler抓包工具 data = { 'kw' : 'dog' } #使用requests模块的post函数根据指定的url发起一个post请求，post函数返回一个响应对象 #参数1：发起请求指定的url #参数2：手动定制的请求头信息 #参数3：指定请求携带的参数 response = requests.post(url=url,headers=headers,data=data) #获取响应内容：响应内容为json串 print(response.text) 其他请求&ensp;&ensp;&ensp;&ensp;Requests 的功能那么强大当然不仅仅限于 GET 和 POST 请求啦，其他一些请求例如 put 请求、delete 请求、head 请求、option 请求等其实都是类似的，但是平时用的不多，就不仔细介绍了，有用到的可以去看官网文档哦。 后话下一张的内容就是 BeautifulSoup 库，我们的解析库！本文借鉴于：Requests 库介绍和基础入门]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 基础爬虫（一）介绍篇]]></title>
    <url>%2F2020%2F06%2F04%2Fpython%2Fintroduce%2F</url>
    <content type="text"><![CDATA[前言&ensp;&ensp;&ensp;&ensp;其实说实话呢，博主也是正在学习 Python 这门语言（主要还是学习网络爬虫这一块），博主主职还是 Java 开发，这次学习也是直接略过了基础这一块直接上手的爬虫，可能会有讲不到或者有不对的地方，请多多包含，欢迎大佬们在评论区给一些建议！！感谢！！下面的介绍主要还是关于爬虫相关的内容 什么是爬虫？&ensp;&ensp;&ensp;&ensp;爬虫全称：网络爬虫（又称为网页蜘蛛，网络机器人，在 FOAF 社区中间，更经常的称为网页追逐者），是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本。另外一些不常使用的名字还有蚂蚁、自动索引、模拟程序或者蠕虫。简单来讲爬虫就相当于是一个探测器，它最常用的操作就是模拟人的行为去各个网站溜达，点点按钮，查查数据，或者把看到的信息背回来。就像一只虫子在一幢楼里不知疲倦地爬来爬去。 爬虫最重要的三个特点！&ensp;&ensp;&ensp;&ensp;1.抓取数据：相当于你打开百度搜索出来的东西，其实都是通过爬虫抓取过来的。&ensp;&ensp;&ensp;&ensp;2.分析数据：相当于根据你输入的条件，筛选出来你想要的信息内容。&ensp;&ensp;&ensp;&ensp;3.存储数据：相当于把你想要的信息，存放到你的数据库里，或者文本里。 爬虫原理&ensp;&ensp;&ensp;&ensp;网络连接像是在自助饮料售货机上购买饮料一样：购买者只需选择所需饮料，投入硬币（或纸币），自助饮料售货机就会弹出相应的商品。&ensp;&ensp;&ensp;&ensp;了解网络连接的基本原理后，爬虫原理就很好理解了。网络连接需要电脑一次 Requests 请求和服务器端的 Response 回应。爬虫也是需要这两件事：&ensp;&ensp;&ensp;&ensp;（1）模拟电脑对服务器发起 Requests 请求。&ensp;&ensp;&ensp;&ensp;（2）接收服务器端的 Response 的内容并解析提取所需信息。 网页构造&ensp;&ensp;&ensp;&ensp;打开任意浏览器，然后随便找到一个网页，例如 CSDN 右键点击查看网页源代码，可以看到一个 HTML 的网页，我们的爬虫其实就是获取的 HTML 里边的信息&ensp;&ensp;&ensp;&ensp;通过这个图，我们可以看到网页的”真实面目”,上半部分是我们引入的一些 Js 文件，下半部分则是一些网页中的内容，浏览器存在的作用就相当于一个翻译官一样，把我们的写的内容翻译成用户使用的网页界面 学习爬虫的必备知识 HTML ： 这个能够帮助你了解网页的结构，内容等。W3School的教程 Python ： 如果有编程基础的小伙伴儿，推荐看一个廖雪峰的Python教程就够了，没有编程基础的小伙伴，推荐看看视频教程，或者可以看看知乎-如何系统的自学Python TCP/IP协议，HTTP协议 ：这些知识能够让你了解在网络请求和网络传输上的基本原理，了解就行，能够帮助今后写爬虫的时候理解爬虫的逻辑 后话本文借鉴于：阿里波特-Python 爬虫 和 维基百科、_tu-爬虫原理和网页构造]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初来乍到的洛尘]]></title>
    <url>%2F2019%2F06%2F11%2Fhello-world%2F</url>
    <content type="text"><![CDATA[自我介绍&ensp;&ensp;&ensp;&ensp;我呢，是一个。。额。。咋形容呢，就是。。是一个偶尔发呆偶尔犯傻的一个孩子，今年呢 18 岁，嘿嘿，进入 IT 行业也已经一年多了，在没进来，这个行业之前觉得，哇塞，这个行业真的很神奇，很有神秘感，但是事实告诉我以上仅限想象行为。&ensp;&ensp;&ensp;&ensp;其实因为年龄问题，我受到过很多的质疑，咱也不知道年龄和技术有啥关系，咱也不敢问。当时求职的时候就感觉超级沮丧，因为受到很多的”打击”，曾经也一度想过放弃，但是吧，放弃又很不甘心，幸好皇天不负有心人，让我幸运的进入到了第一家公司，当时进去的时候真的是特别特别的认真学，大概就是上班吸收不了的知识晚上下班回家继续吸收，一天有二十四小时，真的是除了睡觉的时候，其余时间都是在学习，当然，我的努力也没有白费。&ensp;&ensp;&ensp;&ensp;在入职的第三个月的时候，我负责带一个项目，那是我第一次带领做项目，但是的心情吧，就是，咋说呢，很复杂！！一方面是激动，一方面是害怕，害怕是因为，我害怕让信任我的人失望，激动是因为，这种机会给了一个刚入职的菜鸟，主管都这么信任我了，如果我再让他失望，我觉得我真的就太失败了。我是个很感性的人，就是那种，别人对我好信任我，我就会百分之二百的回报给他的那种。&ensp;&ensp;&ensp;&ensp;在这个公司，我遇到了很多暖心的哥哥，其实职场上也没有那么”恐怖”，以前没进入社会的时候，都说怎么怎么样怎么怎么样，但是这种事情也只有亲身体验了才会知道，反正我感觉我很幸运，遇到的都是很贴心很暖的哥哥们（感觉以前就是电视剧看多了，给“吓”得），虽然偶尔毒舌，偶尔调皮！！但是我向他们请教问题，重来都不会说是不给讲，态度不好什么什么的，我们偶尔也会打闹，后来因为某些事情跳槽了。&ensp;&ensp;&ensp;&ensp;第二家公司其实也是朋友推荐才去的，刚开始的时候他们都感觉，哇，好小的小孩子，后来开始慢慢相处，从哥哥玩成朋友，记得听他们说的最多的一句话就是，我从来没有想过会和 00 后玩的那么好，后来曾经有一点时间特别想学习 Linux 但是由于自身条件不是很优秀，脑子有点笨，学了一段时间，感觉好难，然后公司里的运维哥哥就过来趁下班的时候帮我学习 Linux 真的特别特别感动，虽然每次教我的时候都喜欢’凶’我！！其实现在回想起来还是会忍不住的想要笑，记得他当时一直和我玩游戏还和女朋友发生过’争执’虽然我吃了不少狗粮 T_T||&ensp;&ensp;&ensp;&ensp;我不知道其他人的路是什么样子的，但是我觉得我真的是非常幸运了，每次遇到的人都很好，也许曾经会发生一些不愉快的事情，但是在我心里还是和他们的友情高于一切，当时走的时候，记得我一个很好的哥哥还给我发了一段朋友圈说，祝我以后有酒有肉有姑娘，想得到的都会得到，真的是给我差点感动哭了，一瞬间就是那种千言万语堵在了喉咙的感觉，什么话都说不出来，到了现在也很想念他们，但是每个人都有自己的路要走，虽然感谢和他们相遇，但是却不会再回去了，只期待后面有缘在相遇。&ensp;&ensp;&ensp;&ensp;我没有多么华丽的语言来形用我这不算精彩的精彩人生，虽然我不信命，但是我依旧很感谢幸运女神给我的照顾，依旧感谢与这些哥哥的相遇。其实这篇文章，并没有写一些关于工作的事情，写的相当于这一年多的缩影吧，经历了很多也成长了很多，开心过，高兴过，也颓废过，但是不管怎么样，都要做最好的自己的，坚信一句话，不忘初心，方得始终！！！]]></content>
      <categories>
        <category>linluochen</category>
      </categories>
      <tags>
        <tag>linluochen</tag>
      </tags>
  </entry>
</search>
