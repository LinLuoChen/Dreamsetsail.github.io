<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Python 基础爬虫（二）Requests库]]></title>
    <url>%2F2020%2F06%2F04%2Fpython%2Frequests%2F</url>
    <content type="text"><![CDATA[前言&ensp;&ensp;&ensp;&ensp;Requests 是一常用的 HTTP 请求库，它使用 Python 语言编写，基于 urllib，采用 Apache2 Licensed 开源协议的 HTTP 库，可以方便地发送 HTTP 请求，以及方便地处理响应结果（用了 Requests 之后，你基本都不愿意用 urllib 了） 运行环境&ensp;&ensp;&ensp;&ensp;博主使用的是 IDE 系列的 PyCharm 一个很好用的神器（不是打广告！！是真的好用！！！） 安装方式&ensp;&ensp;&ensp;&ensp;1.windows + R 输入 cmd 右键管理员运行，输入 pip insrall requests&ensp;&ensp;&ensp;&ensp;2.使用 Anaconda 版本的朋友 在命令窗口输入 conda install requests&ensp;&ensp;&ensp;&ensp;3.使用 PyCharm 进行安装 file-》default settings-》project interpreter-》搜索 requests-》install package-》ok 使用方式&ensp;&ensp;&ensp;&ensp;打开我们的 PyCharm 工具，新建一个 Python 文件，在目录右键新建一个 Python 文件，然后输入文件名称，然后回车就行了 先写一个最简单的爬虫（GET 请求）&ensp;&ensp;&ensp;&ensp;我们的第一行代码首先是导入刚刚引入的 requests 库 import requests &ensp;&ensp;&ensp;&ensp;输入想要访问的目标网址并且获取他的响应状态，直接输入 requests.get(&#39;目标网址&#39;) 默认返回的是响应状态和响应对象 requests.get('http://www.baidu.com') # 要访问的目标网址 print(requests.get('http://www.baidu.com')) # 输出目标网址的响应状态 &ensp;&ensp;&ensp;&ensp;完整代码如下，这个 if __name__ == &quot;__main__&quot;: 相当于 Java 里边的 main 方法 import requests if __name__ == "__main__": #指定请求的url地址 url = 'http://www.baidu.com' #使用requests模块的get函数根据指定的url发起一个get请求，get函数返回一个响应对象 response = requests.get(url) #打印响应对象（结果：响应对象类型和响应状态码） print(response) 属性介绍&ensp;&ensp;&ensp;&ensp;使用 requests 模块向百度首页面发起一个 get 请求，获取响应对象 import requests if __name__ == "__main__": #指定请求的url url = 'http://www.baidu.com' #使用requests模块的get函数根据指定的url发起一个get请求，get函数返回一个响应对象 response = requests.get(url) #获取请求的url print('请求的url:'+response.url) #获取响应状态码 print(response.status_code) #获取响应内容的编码格式,可以通过该属性修改响应的编码格式，直接以赋值的形式就可以修改响应的编码方式，一般出现乱码是我们需要进行设置 print('响应内容编码：'+response.encoding) #获取响应对象的响应头信息： print(response.headers) #获取字符串形式的响应内容，即是我们通过审查元素看到的HTML内容 print(response.text) #获取字节形式的响应内容，是bytes类型，一般我们请求图频、音频、视频需要用到字节流形式的响应内容 print(response.content) 设置响应头&ensp;&ensp;&ensp;&ensp;在请求头中有一个参数为User-Agent，表示含义为请求载体的身份标识。通过浏览器发起的请求，请求载体为浏览器，则该请求的User-Agent为浏览器的身份标识，使用爬虫程序发起的请求，则该请求的载体为爬虫程序，则该请求的User-Agent为爬虫程序的身份标识。爬虫程序想要尽可能的模拟浏览器发起的请求，则必须将User-Agent修改成浏览器的身份标识。 注意：User-Agent的定制其实是一种反反爬虫的初级技术手段。所以在编写爬虫程序时，必须对User-Agent进行手动定制。User-Agent的值可以通过抓包工具从浏览器请求的请求头中获取。 import requests if __name__ == "__main__": #指定请求的url url = 'http://www.baidu.com' #定制请求头信息，相关的头信息必须封装在字典结构中 headers = { #定制请求头中的User-Agent参数，当然也可以定制请求头中其他的参数 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36', } #使用requests模块的get函数根据指定的url发起一个get请求，get函数返回一个响应对象 #参数1：发起请求指定的url #参数2：手动定制的请求头信息 response = requests.get(url=url,headers=headers) #打印响应的状态码 print(response.status_code) 携带参数的 GET 请求注意：url有一个显著的特性。url必须是使用ASCII进行编码的字符方可生效。但是在requests模块中，即使url中存在非ASCII字符，那么requests会对其自动进行转化。 import requests if __name__ == "__main__": #指定请求的url：可以将get请求携带的参数拼接到url域名后面，但是不建议大家这么做，一般我们将携带的参数以键值对的形式封装到字典中，操作见下方： #url = 'http://www.baidu.com/s?ie=utf-8&amp;wd=周杰伦' #建议做法：指定请求网页的域名 url = 'https://www.baidu.com/s' #将请求携带的参数以键值对的形式封装到字典中，准备待用 param = { 'ie' : 'utf-8', 'wd' : '星星' } #定制请求头信息，相关的头信息必须封装在字典结构中 headers = { #定制请求头中的User-Agent参数，当然也可以定制请求头中其他的参数 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36', } #使用requests模块的get函数根据指定的url发起一个get请求，get函数返回一个响应对象，get中传入的三个参数如下： #参数1：发起请求指定的url #参数2：手动定制的请求头信息 #参数3：指定请求携带的参数 response = requests.get(url=url,headers=headers,params=param) #设置响应内容的编码格式（如果写入文件内容出现乱码的话） response.encoding = 'utf-8' #获取响应的网页内容，存储的文件名我们也可以根据用户输入，存成一个动态文件名 page_data = response.text with open('./星星.html','w') as fp: fp.write(page_data) 写一个最简单的爬虫（POST 请求）携带参数的 POST 请求注意：这个地方有一个小坑，使用 GET 方法带参数请求时，是 params=参数字典，而不是 data，data 是POST 方法的参数 import requests if __name__ == "__main__": #指定post请求的url url = 'https://fanyi.baidu.com/sug' #定制请求头信息，相关的头信息必须封装在字典结构中 headers = { #定制请求头中的User-Agent参数，将爬虫伪装的更像浏览器，当然也可以定制请求头中其他的参数 'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36', } #定制post请求携带的参数,可以使用抓包工具看需要传哪些参数，如Fiddler抓包工具 data = { 'kw' : 'dog' } #使用requests模块的post函数根据指定的url发起一个post请求，post函数返回一个响应对象 #参数1：发起请求指定的url #参数2：手动定制的请求头信息 #参数3：指定请求携带的参数 response = requests.post(url=url,headers=headers,data=data) #获取响应内容：响应内容为json串 print(response.text) 其他请求&ensp;&ensp;&ensp;&ensp;Requests 的功能那么强大当然不仅仅限于 GET 和 POST 请求啦，其他一些请求例如 put 请求、delete 请求、head 请求、option 请求等其实都是类似的，但是平时用的不多，就不仔细介绍了，有用到的可以去看官网文档哦。 后话下一张的内容就是 BeautifulSoup 库，我们的解析库！本文借鉴于：Requests 库介绍和基础入门]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python 基础爬虫（一）介绍篇]]></title>
    <url>%2F2020%2F06%2F04%2Fpython%2Fintroduce%2F</url>
    <content type="text"><![CDATA[前言&ensp;&ensp;&ensp;&ensp;其实说实话呢，博主也是正在学习 Python 这门语言（主要还是学习网络爬虫这一块），博主主职还是 Java 开发，这次学习也是直接略过了基础这一块直接上手的爬虫，可能会有讲不到或者有不对的地方，请多多包含，欢迎大佬们在评论区给一些建议！！感谢！！下面的介绍主要还是关于爬虫相关的内容 什么是爬虫？&ensp;&ensp;&ensp;&ensp;爬虫全称：网络爬虫（又称为网页蜘蛛，网络机器人，在 FOAF 社区中间，更经常的称为网页追逐者），是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本。另外一些不常使用的名字还有蚂蚁、自动索引、模拟程序或者蠕虫。简单来讲爬虫就相当于是一个探测器，它最常用的操作就是模拟人的行为去各个网站溜达，点点按钮，查查数据，或者把看到的信息背回来。就像一只虫子在一幢楼里不知疲倦地爬来爬去。 爬虫最重要的三个特点！&ensp;&ensp;&ensp;&ensp;1.抓取数据：相当于你打开百度搜索出来的东西，其实都是通过爬虫抓取过来的。&ensp;&ensp;&ensp;&ensp;2.分析数据：相当于根据你输入的条件，筛选出来你想要的信息内容。&ensp;&ensp;&ensp;&ensp;3.存储数据：相当于把你想要的信息，存放到你的数据库里，或者文本里。 爬虫原理&ensp;&ensp;&ensp;&ensp;网络连接像是在自助饮料售货机上购买饮料一样：购买者只需选择所需饮料，投入硬币（或纸币），自助饮料售货机就会弹出相应的商品。&ensp;&ensp;&ensp;&ensp;了解网络连接的基本原理后，爬虫原理就很好理解了。网络连接需要电脑一次 Requests 请求和服务器端的 Response 回应。爬虫也是需要这两件事：&ensp;&ensp;&ensp;&ensp;（1）模拟电脑对服务器发起 Requests 请求。&ensp;&ensp;&ensp;&ensp;（2）接收服务器端的 Response 的内容并解析提取所需信息。 网页构造&ensp;&ensp;&ensp;&ensp;打开任意浏览器，然后随便找到一个网页，例如 CSDN 右键点击查看网页源代码，可以看到一个 HTML 的网页，我们的爬虫其实就是获取的 HTML 里边的信息&ensp;&ensp;&ensp;&ensp;通过这个图，我们可以看到网页的”真实面目”,上半部分是我们引入的一些 Js 文件，下半部分则是一些网页中的内容，浏览器存在的作用就相当于一个翻译官一样，把我们的写的内容翻译成用户使用的网页界面 学习爬虫的必备知识 HTML ： 这个能够帮助你了解网页的结构，内容等。W3School的教程 Python ： 如果有编程基础的小伙伴儿，推荐看一个廖雪峰的Python教程就够了，没有编程基础的小伙伴，推荐看看视频教程，或者可以看看知乎-如何系统的自学Python TCP/IP协议，HTTP协议 ：这些知识能够让你了解在网络请求和网络传输上的基本原理，了解就行，能够帮助今后写爬虫的时候理解爬虫的逻辑 后话本文借鉴于：阿里波特-Python 爬虫 和 维基百科、_tu-爬虫原理和网页构造]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
</search>
